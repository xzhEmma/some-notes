# -*- coding: utf-8 -*-
"""AutoEnconder

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1u34YLpHI943Lgb-eol7_9rue2pC9XyNG

导入必要的包
"""

import numpy as np
import torch
from torchvision.transforms import ToPILImage
import torch.nn as nn 
import torch.utils.data as Data
import os
from PIL import Image
import scipy.misc
import matplotlib.pyplot as plt
import torchvision
from torchvision import datasets, transforms
from google.colab.patches import cv2_imshow

"""设置超参数"""

EPOCH = 5
BATCH_SIZE = 128
LR = 0.005
if os.path.exists('mnist/'):
  DOWNLOAD_MNIST = False
else:
  DOWNLOAD_MNIST = True
N_TEST_IMG = 5

"""导入数据并可视化"""

train_data = torchvision.datasets.MNIST(
    root = './mnist/',
    train = True,
    transform = torchvision.transforms.ToTensor(),
    download = DOWNLOAD_MNIST,
)
test_data = torchvision.datasets.MNIST(
    root = './mnist/',
    train = False,
    transform = torchvision.transforms.ToTensor(),
    download = DOWNLOAD_MNIST
)

#50samples,1channel,28*28
train_loader = Data.DataLoader(
    dataset = train_data,
    batch_size = BATCH_SIZE,
    shuffle = True,
    num_workers = 0
)
test_loader = Data.DataLoader(
    dataset = test_data,
    batch_size = BATCH_SIZE,
    shuffle = True,
    num_workers = 0
)
images,labels = next(iter(train_loader))
#images.shape 128 1 28 28 
img = torchvision.utils.make_grid(images)
#3,482,242
img = img.numpy().transpose(1,2,0)
#482,242,3
std = [0.5,0.5,0.5]
mean = [0.5,0.5,0.5]
img = img* std+ mean
#482,242,3
print([labels[i] for i in range(BATCH_SIZE)])
plt.imshow(img)

"""定义自编码器并训练"""

class AutoEncoder(nn.Module):
    def __init__(self):
        super(AutoEncoder, self).__init__()
        # 编码过程
        self.encoder = nn.Sequential(
            nn.Conv2d(1,32,3,1),
            nn.ReLU(),
            nn.MaxPool2d(2,2),
            nn.Conv2d(32,64,3,1),
            nn.ReLU(),
            nn.MaxPool2d(2,2),
            nn.Conv2d(64,128,3,1),
            nn.ReLU(),
            nn.Conv2d(128,64,3,1),
            nn.ReLU()
        )
        # 解码
        # s(n-1)+k-2p
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(64,128, 3,1), # 3
            nn.ReLU(),
            nn.ConvTranspose2d(128,64,3,1), # 5
            nn.ReLU(),
            nn.ConvTranspose2d(64,64,3,2), # 11
            nn.ReLU(),
            nn.ConvTranspose2d(64,32, 3,1), # 13
            nn.ReLU(),
            nn.ConvTranspose2d(32,32, 3 , 2), # 27
            nn.ReLU(),
            nn.ConvTranspose2d(32,1,2,1)
        )

    def forward(self, x):
        x = self.encoder(x)
        return self.decoder(x)


class Trainer:
    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.net = AutoEncoder().to(self.device)
        self.loss_fn = nn.MSELoss()
        self.opt = torch.optim.Adam(self.net.parameters())
        self.BATCH_SIZE = BATCH_SIZE
        self.EPOHS = EPOCH
    def getTrain_set(self):
        train_set = torchvision.datasets.MNIST(root="./MNIST", train=True)
        origin_x = train_set.data.float()
        train_x = torch.clamp(origin_x + torch.randn(60000, 28, 28) * 100, 0, 255) / 255.
        train_y = origin_x / 255.
        return train_x.unsqueeze(1), train_y.unsqueeze(1)#(N,H,W)加在1

    def getTest_set(self):
        test_set = torchvision.datasets.MNIST(root="./MNIST", train=False,download=True)
        origin_x = test_set.data.float()
        test_x = torch.clamp(origin_x + torch.randn(10000, 28, 28) * 100, 0, 255) / 255.
        test_y = origin_x / 255.
        return test_x.unsqueeze(1), test_y.unsqueeze(1)
    def train(self):
        train_x, train_y = self.getTrain_set()
        train_x = train_x.to(self.device)
        train_y = train_y.to(self.device)

        # 打乱数据
        index = torch.randperm(train_x.shape[0])
        train_x = train_x[index]
        train_y = train_y[index]

        losses = []
        for i in range(self.EPOHS):
            for j in range(0,train_x.shape[0],self.BATCH_SIZE):
                x = train_x[j:j+self.BATCH_SIZE]
                y = train_y[j:j+self.BATCH_SIZE]
                out = self.net(x)
                loss = self.loss_fn(out,y)
                if j%10 == 0:
                    print("epohs:[{}],iteration:[{}]/[{}],loss:{}".format(i,j,60000,loss.float()))
                    losses.append(loss.float())
                    plt.clf()
                    plt.title("loss_removenoise")
                    plt.plot(losses)
                    plt.pause(0.01)
                    plt.savefig("loss_removenoise.jpg")
                self.opt.zero_grad()
                loss.backward()
                self.opt.step()

            torch.save(self.net, "models/removenoise.pth")

    def predictModel(self):
        net = torch.load("models/removenoise.pth")
        test_x, test_y = self.getTest_set()
        test_x = test_x.to(self.device)
        toPIL = ToPILImage("L")
        for i in range(0, test_x.shape[0], self.BATCH_SIZE):
            x = test_x[i:i + self.BATCH_SIZE]
            y = test_y[i:i + self.BATCH_SIZE]
            out = net(x)
            out = torch.sigmoid(out)
            for j in range(out.shape[0]):
                plt.clf()
                plt.subplot(1, 3, 1)
                plt.title("Noise Image")
                plt.axis("off")
                plt.imshow(toPIL(x[j].cpu()))
                plt.subplot(1, 3, 2)
                plt.title("Out Image")
                plt.axis("off")
                plt.imshow(toPIL(out[j].cpu()))
                plt.subplot(1, 3, 3)
                plt.title("Origin Image")
                plt.axis("off")
                plt.imshow(toPIL(y[j]))
                plt.pause(1)
if __name__ == '__main__':
    t = Trainer()
    #t.train()
    t.predictModel()


#optimizer = torch.optim.Adam(autoencoder.parameters(),lr = LR)
#loss_func = nn.MSELoss()
##########################################

"""for epoch in range(EPOCH):
  for step, (x, b_label) in enumerate(train_loader):
    b_x = x.view(-1,28*28)
    #print(b_x.shape,b_x.type)
# 加入噪声

    noise_factor = 0.5
    noisy_imgs = b_x + noise_factor #* np.random.random_sample(b_x.shape)
    
    #noisy_imgs = np.clip(noisy_imgs, 0, 1)
    #print(noisy_imgs.shape,noisy_imgs.type)
    encoded_x, decoded_x = autoencoder(noisy_imgs)
    loss = loss_func(decoded_x,b_x)
  
    optimizer.zero_grad()
    loss.backward()
    optimizer.step() 
    print("Epoch: {}/{} ".format(epoch+1, EPOCH),
             "Training loss: {:.4f}".format(loss))"""


